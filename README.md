# Hardware and Software Optimization

This repository contains small, focused experiments comparing standard Python benchmarks (from pyperformance) with hand-written or alternative implementations, plus tooling to profile them with Linux perf and visualize results via FlameGraphs.

## Current Repository Structure

```
Scripts/
  bash_script.sh
  fast_crypto.py
  orjson_pyperf_bench.py
  script_crypto_pyaes.sh
reports/
  Final reports/
    report_crypto_pyaes.txt
    report_json_dumps.txt
  Flamegraph/
    fast.svg
    flamegraph.svg
    pyaes.svg
  fast.json
  fast_<timestamp>.json
  fast_report.txt
  perf_diff.txt
  pyaes.json
  pyaes_<timestamp>.json
  pyaes_report.txt
  timing_delta.txt
venv/               (local virtual environment, not required to exist in repo)
.gitignore
README.md
```

### Scripts

- `fast_crypto.py`
  AES‑CTR encrypt+decrypt micro-benchmark implemented with the `cryptography` (OpenSSL) backend. Supports two modes:
  - Pyperf mode (when invoked with benchmark-style args): integrates with `pyperf` for statistically robust runs.
  - Plain mode (no dash args): tight loop useful for `perf record` sampling.

- `script_crypto_pyaes.sh`
  Orchestrates comparison between the official `pyperformance` benchmark `crypto_pyaes` and the optimized `fast_crypto.py` implementation.
  Steps performed:
  1. Records perf samples for `crypto_pyaes`.
  2. Records perf samples for `fast_crypto.py` (with a large iteration count via `FAST_LOOPS`).
  3. Generates folded stacks and FlameGraphs (expects a `FlameGraph/` directory at repo root—see below).
  4. Runs both benchmarks to JSON result files and produces a side-by-side comparison using `pyperf compare`.

- `orjson_pyperf_bench.py`
  Apples-to-apples benchmark of `orjson.dumps` using the exact payload shapes defined by the `pyperformance` JSON benchmark (EMPTY, SIMPLE, NESTED, HUGE). Allows comparing high-performance JSON serialization vs stdlib.

- `bash_script.sh`
  Convenience script to:
  1. Run and profile the stdlib `json_dumps` (through `pyperformance`).
  2. Run and profile the custom `orjson` benchmark.
  3. Generate separate FlameGraphs (`flamegraph_json.svg`, `flamegraph_orjson.svg`) for quick visual inspection.
  Paths inside may expect FlameGraph utilities to be installed under `~/FlameGraph` (adjust `FLAMEGRAPH_DIR` if different).

### Reports Directory

Artifacts generated by the benchmarking / profiling scripts:

- `Flamegraph/`
  - `fast.svg`: FlameGraph of the optimized AES benchmark (`fast_crypto.py`).
  - `pyaes.svg`: FlameGraph of the reference `crypto_pyaes` run.
  - `flamegraph.svg`: Generic or consolidated flame graph (naming may vary depending on script usage).

- `Final reports/`
  - `report_crypto_pyaes.txt`: Curated or summarized observations from AES benchmarking.
  - `report_json_dumps.txt`: Summary notes / analysis for JSON serialization benchmarks.

- Root of `reports/`:
  - `fast.json`, `pyaes.json`: Primary pyperf result files for fast vs reference AES benchmarks.
  - `fast_<timestamp>.json`, `pyaes_<timestamp>.json`: Timestamped historical captures (e.g., `fast_20250825-165541.json`).
  - `fast_report.txt`, `pyaes_report.txt`: Human-readable dumps or extracted summaries from the JSON result files.
  - `perf_diff.txt`: Output from `pyperf compare` showing relative performance differences (speedups, confidence intervals).
  - `timing_delta.txt`: Additional derived stats (e.g., delta calculations between runs) if generated by post-processing.

### Virtual Environment

`venv/` is a conventional Python virtual environment directory. It is optional and typically excluded from version control. Recreate locally as needed (instructions below).

## Dependencies

Core Python dependencies (declared in `requirements.txt`):
- `pyperf` – Deterministic micro-benchmark harness.
- `pyperformance` – Official suite providing reference benchmarks (e.g., `crypto_pyaes`, `json_dumps`).
- `cryptography` – OpenSSL-backed primitives for the optimized AES benchmark.
- `orjson` – High-performance JSON serialization library.
- `numpy`, `matplotlib` – (Optional) For analysis / visualization if you add plotting scripts.

System / tooling dependencies:
- Linux `perf` – Sampling profiler used by provided shell scripts.
- FlameGraph utilities – Clone from https://github.com/brendangregg/FlameGraph (either into `./FlameGraph` or adjust script paths / environment variable).

## Setup

```bash
# 1. Clone
git clone https://github.com/Beng1997/hw_sw_opt.git
cd hw_sw_opt

# 2. (Optional) Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. (Optional) Get FlameGraph tools
git clone https://github.com/brendangregg/FlameGraph ./FlameGraph
# or place them elsewhere and edit the scripts (FLAMEGRAPH_DIR)
```

To allow non-root perf (optional, security-sensitive):
```bash
sudo sysctl -w kernel.perf_event_paranoid=1
sudo sysctl -w kernel.kptr_restrict=0
```

## Running Benchmarks & Profiling

### AES Benchmarks & Comparison

```bash
cd Scripts
./script_crypto_pyaes.sh
```
Outputs (into `reports/`): perf data (intermediate), flame graphs, JSON result files, `perf_diff.txt` comparison.

Run optimized AES standalone (pyperf mode):
```bash
python fast_crypto.py -o fast.json
```
Plain mode (tight loop; good for quick perf sampling):
```bash
python fast_crypto.py
```

### JSON Serialization Benchmarks

Stdlib benchmark (pyperformance):
```bash
python -m pyperformance run --bench json_dumps -o reports/pyaes.json  # adjust output name if desired
```
`orjson` apples-to-apples benchmark:
```bash
python Scripts/orjson_pyperf_bench.py -o reports/orjson.json
```

FlameGraph generation for JSON (combined script):
```bash
cd Scripts
./bash_script.sh
```
Creates `flamegraph_json.svg` and `flamegraph_orjson.svg` (paths/names shown in console output). If those need to live under `reports/Flamegraph/`, move or adjust the script.

## Interpreting Results

- Use `pyperf stats <file.json>` for individual run stats.
- Use `pyperf compare --table fast.json pyaes.json` to display relative speed differences.
- Inspect `perf_diff.txt` for stored comparisons.
- FlameGraphs highlight hotspots; compare `fast.svg` vs `pyaes.svg` to see shifts in CPU time distribution.

## Adding New Experiments

1. Create a new script under `Scripts/` following the pyperf pattern (define a function and register with `pyperf.Runner`).
2. Add a profiling stanza (similar to existing shell scripts) if low-level analysis is needed.
3. Emit results to `reports/` using consistent naming (`<name>.json`, `<name>_report.txt`).
4. Update this README if structure changes (e.g., new subdirectories or categories).

## Troubleshooting

| Issue | Possible Cause / Fix |
|-------|----------------------|
| `perf: Permission denied` | Run with `sudo` or relax `perf_event_paranoid`. |
| `python3-dbg: command not found` | Install a debug build (e.g., `sudo apt install python3-dbg`) or edit scripts to use `python3`. |
| Missing FlameGraph scripts | Ensure `FlameGraph/` directory exists or update paths in shell scripts. |
| Large variance in results | Ensure minimal background load; pin CPU affinity or increase warmups/loops. |

## Roadmap / Possible Enhancements

- Add C or Rust extension baselines for AES / JSON (if pursued, introduce an `extensions/` directory and update this doc).
- Automate chart generation (e.g., parse JSON outputs into plots under `reports/`).
- Continuous benchmarking workflow (GitHub Actions + artifact upload) for regression tracking.

## License

(Choose and add a license, e.g., MIT, Apache-2.0, etc.)

---
Last updated to reflect actual on-disk structure shown in the repository (Scripts directory contents and reports layout).